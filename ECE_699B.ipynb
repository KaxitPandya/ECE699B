{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XqcAm7KFm7N"
      },
      "outputs": [],
      "source": [
        "# Dataset: https://data.cincinnati-oh.gov/Thriving-Neighborhoods/Fleet-Preventative-Maintenance-Repair-Work-Orders/2a8x-bxjm/about_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M7P0hwmbvl-",
        "outputId": "7cfee755-c058-408b-b17c-cfdf5ed89f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft==0.8.2 in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (2.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (1.7.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.8.2) (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2023.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch>=1.13.0->peft==0.8.2)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.8.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.8.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.8.2) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2025.4.26)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Installing collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "Successfully installed triton-3.2.0\n",
            "Requirement already satisfied: bitsandbytes==0.42.0 in /usr/local/lib/python3.11/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.42.0) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->bitsandbytes==0.42.0) (2.0.2)\n",
            "Requirement already satisfied: datasets==2.17.1 in /usr/local/lib/python3.11/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.1) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.17.1) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.17.1) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.17.1) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.17.1) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.17.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.17.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.17.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.17.1) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.17.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.17.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.17.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.1) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.7.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Collecting torch==2.6.0 (from torchvision)\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2023.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0->torchvision)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Installing collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.3.1\n",
            "    Uninstalling triton-3.3.1:\n",
            "      Successfully uninstalled triton-3.3.1\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.7.1\n",
            "    Uninstalling torch-2.7.1:\n",
            "      Successfully uninstalled torch-2.7.1\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Collecting triton==3.1.0\n",
            "  Obtaining dependency information for triton==3.1.0 from https://files.pythonhosted.org/packages/86/17/d9a5cf4fcf46291856d1e90762e36cbabd2a56c7265da0d1d9508c8e3943/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting filelock (from triton==3.1.0)\n",
            "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl.metadata\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Using cached triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: filelock, triton\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/filelock-3.18.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/filelock/\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Removing file or directory /usr/local/bin/proton\n",
            "      Removing file or directory /usr/local/bin/proton-viewer\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/triton-3.2.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/triton/\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  changing mode of /usr/local/bin/proton to 755\n",
            "  changing mode of /usr/local/bin/proton-viewer to 755\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filelock-3.18.0 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mlflow>=2.11.0\n",
        "!pip install peft==0.8.2\n",
        "!pip install bitsandbytes==0.42.0\n",
        "!pip install datasets==2.17.1\n",
        "!pip install --upgrade transformers>=4.44.0\n",
        "!pip install --upgrade torch\n",
        "!pip install --upgrade accelerate\n",
        "\n",
        "# For data processing and utilities\n",
        "!pip install pandas numpy\n",
        "!pip install torchvision torchaudio\n",
        "!pip install scikit-learn\n",
        "!pip install wandb  # For experiment tracking\n",
        "!pip install requests  # For data loading\n",
        "!pip install psutil  # For monitoring system resources\n",
        "\n",
        "!pip install --force-reinstall -v \"triton==3.1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_cosci9e0zv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set these BEFORE importing any other packages\n",
        "os.environ[\"BNB_CUDA_VERSION\"] = \"123\"  # Use CUDA 12.3 binaries instead\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = f\"{os.environ.get('LD_LIBRARY_PATH', '')}:/usr/local/cuda/lib64\"\n",
        "\n",
        "# Now import your packages\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27lzgIfTfavZ",
        "outputId": "69f45e4e-fd03-416c-dd96-651b3381e7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM5AeJpwsytH",
        "outputId": "5ed1017d-9a1a-4f82-a176-1f164165c44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Jun  8 02:06:17 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             44W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2uI-a9KXkyU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HF_TOKEN=\"\" # Hugging Face token\n",
        "# os.environ[\"WANDB_MODE\"] = \"offline\" # \n",
        "# %env WANDB_MODE=offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKb35LX7Z_c5",
        "outputId": "8a992406-d657-4ae3-ce6b-6e2a3b7601c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory before loading: 0.00 GB\n",
            "GPU memory reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "# Add this at the very beginning of your notebook\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear any existing GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check available memory\n",
        "print(f\"GPU memory before loading: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTXYypxcaGUV"
      },
      "outputs": [],
      "source": [
        "# Set these BEFORE importing transformers\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_CACHING\"] = \"1\"  # Disable caching for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eecBU7loZ91a",
        "outputId": "741dafc5-7e93-4ea6-d061-97b97761142c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250608_021410-jh82mk88</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance/runs/jh82mk88' target=\"_blank\">fleet-maintenance-fp32</a></strong> to <a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance' target=\"_blank\">https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance/runs/jh82mk88' target=\"_blank\">https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance/runs/jh82mk88</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 46:20, Epoch 200/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.285200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.878200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.967100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.216900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.561700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.443800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.370100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.325700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.272300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.251600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.233200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.196800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.188500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.176500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.171500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.165200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.159000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.155700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.148500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.144100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.141000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.140600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.136000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.129400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.129700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.127200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.126100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.124500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.122100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.120200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.137900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▄▆████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>model</td><td>DialoGPT-medium</td></tr><tr><td>precision</td><td>fp32</td></tr><tr><td>status</td><td>completed</td></tr><tr><td>total_flos</td><td>2786102083584000.0</td></tr><tr><td>train/epoch</td><td>200</td></tr><tr><td>train/global_step</td><td>200</td></tr><tr><td>train/grad_norm</td><td>0.23157</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1379</td></tr><tr><td>train_loss</td><td>0.71982</td></tr><tr><td>train_runtime</td><td>2784.0845</td></tr><tr><td>train_samples_per_second</td><td>2.155</td></tr><tr><td>train_steps_per_second</td><td>0.072</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fleet-maintenance-fp32</strong> at: <a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance/runs/jh82mk88' target=\"_blank\">https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance/runs/jh82mk88</a><br> View project at: <a href='https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance' target=\"_blank\">https://wandb.ai/kaxitpandya-university-of-waterloo/cincinnati-fleet-maintenance</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250608_021410-jh82mk88/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning completed successfully!\n",
            "Final results: {'status': 'completed', 'model': 'DialoGPT-medium', 'precision': 'fp32'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from typing import Dict, List, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CincinnatiFleetDataProcessor:\n",
        "    \"\"\"Advanced data processor for Cincinnati Fleet Maintenance dataset\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_url: str):\n",
        "        self.dataset_url = dataset_url\n",
        "        self.data = None\n",
        "        self.processed_data = None\n",
        "\n",
        "    def load_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load dataset from Cincinnati open data portal\"\"\"\n",
        "        try:\n",
        "            # Cincinnati open data API endpoint\n",
        "            api_url = \"https://data.cincinnati-oh.gov/resource/2a8x-bxjm.json\"\n",
        "\n",
        "            # Load with reduced limit for testing\n",
        "            # params = {\"$limit\": 50000}  # Further reduced for stability\n",
        "            response = requests.get(api_url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "            self.data = pd.DataFrame(data)\n",
        "            logger.info(f\"Successfully loaded {len(self.data)} total records\")\n",
        "\n",
        "            # Print actual columns for debugging\n",
        "            logger.info(f\"Actual columns: {list(self.data.columns)}\")\n",
        "            return self.data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load data: {str(e)}\")\n",
        "            # Create sample data for testing\n",
        "            return self._create_sample_data()\n",
        "\n",
        "\n",
        "    def explore_data(self) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive data exploration\"\"\"\n",
        "        if self.data is None:\n",
        "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
        "\n",
        "        exploration = {\n",
        "            \"shape\": self.data.shape,\n",
        "            \"columns\": list(self.data.columns),\n",
        "            \"dtypes\": self.data.dtypes.to_dict(),\n",
        "            \"missing_values\": self.data.isnull().sum().to_dict(),\n",
        "            \"sample_data\": self.data.head(3).to_dict('records')\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Dataset shape: {exploration['shape']}\")\n",
        "        logger.info(f\"Columns: {exploration['columns']}\")\n",
        "\n",
        "        return exploration\n",
        "\n",
        "    def advanced_serialization(self, row: pd.Series) -> str:\n",
        "        \"\"\"Advanced serialization based on research findings\"\"\"\n",
        "        serialized_parts = []\n",
        "        for col, val in row.items():\n",
        "            if pd.notna(val):\n",
        "                if isinstance(val, (int, float)):\n",
        "                    if isinstance(val, float):\n",
        "                        val = round(val, 3)\n",
        "                    serialized_parts.append(f\"The {col.lower().replace('_', ' ')} is {val}\")\n",
        "                else:\n",
        "                    serialized_parts.append(f\"The {col.lower().replace('_', ' ')} is {str(val)}\")\n",
        "\n",
        "        return \". \".join(serialized_parts) + \".\"\n",
        "\n",
        "class FleetMaintenanceDataset(Dataset):\n",
        "    \"\"\"Custom dataset for fleet maintenance data\"\"\"\n",
        "\n",
        "    def __init__(self, examples: List[Dict[str, str]], tokenizer, max_length: int = 256):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=False,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "        }\n",
        "\n",
        "class AdvancedLLMFineTuner:\n",
        "    \"\"\"FIXED: LLM fine-tuning implementation without FP16 issues\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"FIXED: Initialize model without FP16/LoRA complications\"\"\"\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Add padding token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # FIXED: Load model in float32 to avoid FP16 gradient issues\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float32,  # Force float32 to avoid FP16 issues\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Resize token embeddings if needed\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        logger.info(f\"Model loaded: {self.model_name} (float32)\")\n",
        "\n",
        "    def create_training_args(self, output_dir: str = \"./results\") -> TrainingArguments:\n",
        "        \"\"\"FIXED: Create training arguments without FP16\"\"\"\n",
        "\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "\n",
        "            # Conservative training parameters\n",
        "            num_train_epochs=200,\n",
        "            per_device_train_batch_size=4,  # Very small batch size\n",
        "            per_device_eval_batch_size=4,\n",
        "            gradient_accumulation_steps=32,  # Simulate larger batch\n",
        "\n",
        "            # Optimization settings\n",
        "            learning_rate=3e-5,  # Conservative learning rate\n",
        "            weight_decay=0.01,\n",
        "            warmup_ratio=0.1,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "\n",
        "            # FIXED: Disable FP16 to avoid gradient scaler issues\n",
        "            fp16=False,  # Disabled to fix the error\n",
        "            bf16=False,  # Also disabled\n",
        "            dataloader_pin_memory=False,\n",
        "            gradient_checkpointing=False,\n",
        "\n",
        "            # Evaluation and saving\n",
        "            eval_strategy=\"no\",\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=False,\n",
        "\n",
        "            # Logging\n",
        "            logging_steps=5,\n",
        "            report_to=\"wandb\" if wandb.run else \"none\",\n",
        "            run_name=\"fleet-maintenance-training\",  # Fix the run name warning\n",
        "\n",
        "            # Advanced settings\n",
        "            remove_unused_columns=True,\n",
        "            seed=42,\n",
        "\n",
        "            # FIXED: Disable problematic features\n",
        "            dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
        "            prediction_loss_only=True,  # Simplify loss computation\n",
        "        )\n",
        "\n",
        "    def train(self, train_dataset: Dataset, training_args: TrainingArguments):\n",
        "        \"\"\"FIXED: Execute training without evaluation dataset\"\"\"\n",
        "\n",
        "        # FIXED: Simple data collator without special handling\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "            pad_to_multiple_of=None  # Avoid alignment issues\n",
        "        )\n",
        "\n",
        "        # Initialize trainer with minimal configuration\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "        # FIXED: Clear any existing gradients and ensure proper setup\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        # Start training\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        try:\n",
        "            self.trainer.train()\n",
        "            logger.info(\"Fine-tuning completed successfully!\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Save the final model\n",
        "        self.trainer.save_model()\n",
        "        logger.info(\"Model saved!\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"FIXED: Main execution pipeline\"\"\"\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"cincinnati-fleet-maintenance\", name=\"fleet-maintenance-fp32\")\n",
        "\n",
        "    # Stage 1: Dataset Preparation\n",
        "    logger.info(\"Stage 1: Dataset Preparation\")\n",
        "    processor = CincinnatiFleetDataProcessor(\n",
        "        \"https://data.cincinnati-oh.gov/Thriving-Neighborhoods/Fleet-Preventative-Maintenance-Repair-Work-Orders/2a8x-bxjm\"\n",
        "    )\n",
        "\n",
        "    # Load and explore data\n",
        "    data = processor.load_data()\n",
        "    exploration = processor.explore_data()\n",
        "\n",
        "    # Create training examples\n",
        "    examples = processor.create_training_examples()\n",
        "\n",
        "    # Use only training data for simplicity\n",
        "    train_examples = examples\n",
        "\n",
        "    logger.info(f\"Training examples: {len(train_examples)}\")\n",
        "\n",
        "    # Stage 2: Model Initialization\n",
        "    logger.info(\"Stage 2: Model Initialization\")\n",
        "    fine_tuner = AdvancedLLMFineTuner(\n",
        "        model_name=\"microsoft/DialoGPT-medium\"\n",
        "    )\n",
        "    fine_tuner.setup_model_and_tokenizer()\n",
        "\n",
        "    # Stage 3: Training Environment Setup\n",
        "    logger.info(\"Stage 3: Training Environment Setup\")\n",
        "    train_dataset = FleetMaintenanceDataset(train_examples, fine_tuner.tokenizer)\n",
        "\n",
        "    training_args = fine_tuner.create_training_args(\"./cincinnati_fleet_model\")\n",
        "\n",
        "    # Stage 4: Fine-Tuning\n",
        "    logger.info(\"Stage 4: Fine-Tuning\")\n",
        "    fine_tuner.train(train_dataset, training_args)\n",
        "\n",
        "    # Stage 5: Completion\n",
        "    logger.info(\"Stage 5: Training completed successfully\")\n",
        "    eval_results = {\"status\": \"completed\", \"model\": \"DialoGPT-medium\", \"precision\": \"fp32\"}\n",
        "\n",
        "    wandb.log(eval_results)\n",
        "    wandb.finish()\n",
        "\n",
        "    return fine_tuner, eval_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Execute the complete pipeline\n",
        "    model, results = main()\n",
        "    print(\"Fine-tuning completed successfully!\")\n",
        "    print(f\"Final results: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1xj6P-2nOpt",
        "outputId": "37c4b4b8-fe20-46a9-8811-bb27aa238ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Testing Fine-Tuned Fleet Maintenance Model (FIXED)\n",
            "============================================================\n",
            "🔄 Loading fine-tuned model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ CUDA loading failed: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n",
            "🔄 Using CPU instead...\n",
            "\n",
            "🔍 System Status Check:\n",
            "==================================================\n",
            "PyTorch Version: 2.6.0+cu124\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.4\n",
            "GPU Count: 1\n",
            "Current GPU: 0\n",
            "GPU Name: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory - Allocated: 1.54GB\n",
            "GPU Memory - Reserved: 1.61GB\n",
            "GPU Memory - Total: 39.56GB\n",
            "Model Device: cpu\n",
            "Model Path: ./cincinnati_fleet_model\n",
            "\n",
            "🔧 Testing Custom Fleet Maintenance Inputs:\n",
            "============================================================\n",
            "\n",
            "1. Police Vehicle Maintenance\n",
            "Input: Maintenance: The wo no is PD-2024-001. The equipment no is POLICE-CAR-15. The work type is Preventive Maintenance. The priority is High. The cost is 1500.75. The department is Police.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is PM. The eq equip no is 2023. The work order status is OPEN. The meter 1 reading is 97671. The datetime out service is 2023-----08-07T00:00:00.000Z. The datetime open is 2023----08-07T00:00:00.000Z. The datetime first labor is 2023-----08-07T00:00:\n",
            "----------------------------------------\n",
            "\n",
            "2. Fire Truck Emergency Repair\n",
            "Input: Maintenance: The wo no is FD-2024-002. The equipment no is FIRE-TRUCK-3. The work type is Repair. The priority is Critical. The cost is 8500.00. The department is Fire.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is REPAIR. The eq equip no is 60954. The work order status is OPEN. The meter 1 reading is 97671. The datetime out service is 2023-08-002. The datetime open is 2023-08-003. The datetime first labor is 2023-08-003. The datetime unit in is 2023-08-003. The datetime due is 2023-08-004. The qty est hours\n",
            "----------------------------------------\n",
            "\n",
            "3. Public Works Equipment\n",
            "Input: Maintenance: The wo no is PW-2024-003. The equipment no is SNOW-PLOW-7. The work type is Inspection. The priority is Medium. The cost is 250.00. The department is Public Works.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is REPAIR. The eq equip no is 30842. The work order status is WORK FINISHED. The meter 1 reading is 59384. The datetime out service is 2023-08-003. The datetime open is 2023-08-003. The datetime first labor is 2023-08-003. The datetime unit in is 2023-08-003. The datetime due is 2023-08-04T00:00\n",
            "----------------------------------------\n",
            "\n",
            "📊 Testing Dataset Examples:\n",
            "============================================================\n",
            "\n",
            "1. Dataset Example 1\n",
            "Input: Maintenance: The wo no is WO-1001. The equipment no is EQ-201. The work type is Preventive Maintenance. The priority is High. The cost is 2500.0. The department is Police.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is REPAIR. The eq equip no is 30842. The work order status is WORK FINISHED. The meter 1 reading is 171561. The datetime out service is 2023-08-07T00:00:00.000Z. The datetime open is 2023-08-07T00:00:00.000Z. The datetime first labor unit in is 2023-08-07T00:00:00.000\n",
            "----------------------------------------\n",
            "\n",
            "2. Dataset Example 2\n",
            "Input: Maintenance: The wo no is WO-1002. The equipment no is EQ-202. The work type is Repair. The priority is Medium. The cost is 1200.5. The department is Fire.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is REPAIR. The eq equip no is 30842. The work order status is WORK FINISHED. The meter 1 reading is 171561. The datetime out service is 2023-08-07T00:00:00.000Z. The datetime open is 2023-08-07T00:00:00.000Z. The datetime first labor is 0. The datetime unit in is 2023-08-07T00\n",
            "----------------------------------------\n",
            "\n",
            "3. Dataset Example 3\n",
            "Input: Maintenance: The wo no is WO-1003. The equipment no is EQ-203. The work type is Inspection. The priority is Low. The cost is 300.0. The department is Public Works.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated: The job type is REPAIR. The eq equip no is 62590. The work order status is OPEN. The meter 1 reading is 171561. The datetime out service is 2023-08-07T00:00:00.000Z. The datetime open is 2023-08-07T00:00:00.000Z. The datetime unit in is 2023-08-07T00:00:00.000Z. The datetime\n",
            "----------------------------------------\n",
            "\n",
            "⚡ Performance Benchmark:\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Response Time: 2.468s ± 0.006s\n",
            "Responses per minute: ~24.3\n",
            "Device used: cpu\n",
            "Sample response: The job type is REPAIR. The eq equip no is 37464. The work order status is CLOSED. The meter 1 reading is 171561. The datetime out service is 2023-08-07T00:00:\n",
            "\n",
            "✅ Testing completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# FIXED: Enable CUDA debugging for better error traces\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
        "\n",
        "class FixedFleetMaintenanceInference:\n",
        "    \"\"\"FIXED: Class to test the fine-tuned fleet maintenance model with proper error handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_path=\"./cincinnati_fleet_model\"):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = \"cpu\"  # Start with CPU for safety\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"FIXED: Load model with proper error handling and device management\"\"\"\n",
        "        try:\n",
        "            print(\"🔄 Loading fine-tuned model...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
        "\n",
        "            # FIXED: Proper pad token setup to avoid tokenization issues\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "            # FIXED: Load model on CPU first, then move to GPU if available\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_path,\n",
        "                torch_dtype=torch.float32,\n",
        "                device_map=None  # Load on CPU first\n",
        "            )\n",
        "\n",
        "            # FIXED: Safely move to GPU with error handling\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    self.model = self.model.to(\"cuda\")\n",
        "                    self.device = \"cuda\"\n",
        "                    print(f\"✅ Model loaded on {self.device}\")\n",
        "                except Exception as cuda_error:\n",
        "                    print(f\"⚠️ CUDA loading failed: {cuda_error}\")\n",
        "                    print(\"🔄 Using CPU instead...\")\n",
        "                    self.model = self.model.to(\"cpu\")\n",
        "                    self.device = \"cpu\"\n",
        "            else:\n",
        "                self.device = \"cpu\"\n",
        "                print(f\"✅ Model loaded on {self.device}\")\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading fine-tuned model: {e}\")\n",
        "            print(\"🔄 Falling back to base model...\")\n",
        "            self._load_fallback_model()\n",
        "\n",
        "    def _load_fallback_model(self):\n",
        "        \"\"\"Load base model as fallback\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "            # Move to appropriate device\n",
        "            if torch.cuda.is_available():\n",
        "                try:\n",
        "                    self.model = self.model.to(\"cuda\")\n",
        "                    self.device = \"cuda\"\n",
        "                except:\n",
        "                    self.device = \"cpu\"\n",
        "            else:\n",
        "                self.device = \"cpu\"\n",
        "\n",
        "            print(f\"✅ Fallback model loaded on {self.device}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to load fallback model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_response(self, input_text, max_new_tokens=1000, temperature=0.7):\n",
        "        \"\"\"FIXED: Generate response with proper error handling and token validation\"\"\"\n",
        "\n",
        "        try:\n",
        "            # FIXED: Proper tokenization with attention masks and validation\n",
        "            inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=1024,  # Limit input length\n",
        "                add_special_tokens=True\n",
        "            )\n",
        "\n",
        "            # FIXED: Validate token IDs before moving to device\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "            # Check for invalid token IDs that cause CUDA errors\n",
        "            if torch.any(input_ids >= self.tokenizer.vocab_size):\n",
        "                print(\"⚠️ Invalid token IDs detected, cleaning...\")\n",
        "                input_ids = torch.clamp(input_ids, 0, self.tokenizer.vocab_size - 1)\n",
        "\n",
        "            if torch.any(input_ids < 0):\n",
        "                print(\"⚠️ Negative token IDs detected, cleaning...\")\n",
        "                input_ids = torch.clamp(input_ids, 0, self.tokenizer.vocab_size - 1)\n",
        "\n",
        "            # FIXED: Safely move tensors to device with error handling\n",
        "            try:\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "            except RuntimeError as cuda_error:\n",
        "                print(f\"⚠️ CUDA tensor error: {cuda_error}\")\n",
        "                print(\"🔄 Falling back to CPU...\")\n",
        "                self.device = \"cpu\"\n",
        "                self.model = self.model.to(\"cpu\")\n",
        "                input_ids = input_ids.to(\"cpu\")\n",
        "                attention_mask = attention_mask.to(\"cpu\")\n",
        "\n",
        "            # FIXED: Generate with better parameters and error handling\n",
        "            with torch.no_grad():\n",
        "                try:\n",
        "                    outputs = self.model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_new_tokens=max_new_tokens,  # Use max_new_tokens instead of max_length\n",
        "                        temperature=temperature,\n",
        "                        do_sample=True,\n",
        "                        top_p=0.9,\n",
        "                        top_k=50,\n",
        "                        repetition_penalty=1.2,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        eos_token_id=self.tokenizer.eos_token_id,\n",
        "                        num_return_sequences=1,\n",
        "                        early_stopping=True\n",
        "                    )\n",
        "                except RuntimeError as gen_error:\n",
        "                    print(f\"⚠️ Generation error: {gen_error}\")\n",
        "                    # Fallback to simpler generation\n",
        "                    outputs = self.model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        max_new_tokens=50,\n",
        "                        temperature=0.8,\n",
        "                        do_sample=False,  # Use greedy decoding as fallback\n",
        "                        pad_token_id=self.tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "            # FIXED: Proper response extraction\n",
        "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract only the generated part\n",
        "            if input_text in full_response:\n",
        "                generated_part = full_response[len(input_text):].strip()\n",
        "                return generated_part if generated_part else \"No response generated.\"\n",
        "\n",
        "            return full_response.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error in generate_response: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def test_custom_inputs(self):\n",
        "        \"\"\"Test model on custom fleet maintenance scenarios\"\"\"\n",
        "\n",
        "        print(\"\\n🔧 Testing Custom Fleet Maintenance Inputs:\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        custom_tests = [\n",
        "            {\n",
        "                \"name\": \"Police Vehicle Maintenance\",\n",
        "                \"input\": \"Maintenance: The wo no is PD-2024-001. The equipment no is POLICE-CAR-15. The work type is Preventive Maintenance. The priority is High. The cost is 1500.75. The department is Police.\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Fire Truck Emergency Repair\",\n",
        "                \"input\": \"Maintenance: The wo no is FD-2024-002. The equipment no is FIRE-TRUCK-3. The work type is Repair. The priority is Critical. The cost is 8500.00. The department is Fire.\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"Public Works Equipment\",\n",
        "                \"input\": \"Maintenance: The wo no is PW-2024-003. The equipment no is SNOW-PLOW-7. The work type is Inspection. The priority is Medium. The cost is 250.00. The department is Public Works.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for i, test in enumerate(custom_tests, 1):\n",
        "            print(f\"\\n{i}. {test['name']}\")\n",
        "            print(f\"Input: {test['input']}\")\n",
        "\n",
        "            try:\n",
        "                response = self.generate_response(test['input'])\n",
        "                print(f\"Generated: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    def test_dataset_examples(self):\n",
        "        \"\"\"Test model on examples from the dataset\"\"\"\n",
        "\n",
        "        print(\"\\n📊 Testing Dataset Examples:\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        dataset_examples = [\n",
        "            \"Maintenance: The wo no is WO-1001. The equipment no is EQ-201. The work type is Preventive Maintenance. The priority is High. The cost is 2500.0. The department is Police.\",\n",
        "            \"Maintenance: The wo no is WO-1002. The equipment no is EQ-202. The work type is Repair. The priority is Medium. The cost is 1200.5. The department is Fire.\",\n",
        "            \"Maintenance: The wo no is WO-1003. The equipment no is EQ-203. The work type is Inspection. The priority is Low. The cost is 300.0. The department is Public Works.\"\n",
        "        ]\n",
        "\n",
        "        for i, example in enumerate(dataset_examples, 1):\n",
        "            print(f\"\\n{i}. Dataset Example {i}\")\n",
        "            print(f\"Input: {example}\")\n",
        "\n",
        "            try:\n",
        "                response = self.generate_response(example)\n",
        "                print(f\"Generated: {response}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error: {e}\")\n",
        "\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    def benchmark_performance(self):\n",
        "        \"\"\"Benchmark model performance\"\"\"\n",
        "\n",
        "        print(\"\\n⚡ Performance Benchmark:\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        test_input = \"Maintenance: The wo no is BENCH-001. The equipment no is TEST-VEHICLE. The work type is Preventive Maintenance. The priority is High.\"\n",
        "\n",
        "        import time\n",
        "\n",
        "        # Warm-up\n",
        "        try:\n",
        "            _ = self.generate_response(test_input, max_new_tokens=50)\n",
        "        except:\n",
        "            print(\"⚠️ Warm-up failed, continuing with benchmark...\")\n",
        "\n",
        "        # Benchmark\n",
        "        times = []\n",
        "        responses = []\n",
        "\n",
        "        for i in range(3):  # Reduced iterations for safety\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "                response = self.generate_response(test_input, max_new_tokens=50)\n",
        "                end_time = time.time()\n",
        "                times.append(end_time - start_time)\n",
        "                responses.append(response)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Benchmark iteration {i+1} failed: {e}\")\n",
        "\n",
        "        if times:\n",
        "            avg_time = np.mean(times)\n",
        "            std_time = np.std(times)\n",
        "\n",
        "            print(f\"Average Response Time: {avg_time:.3f}s ± {std_time:.3f}s\")\n",
        "            print(f\"Responses per minute: ~{60/avg_time:.1f}\")\n",
        "            print(f\"Device used: {self.device}\")\n",
        "            if responses:\n",
        "                print(f\"Sample response: {responses[0]}\")\n",
        "        else:\n",
        "            print(\"❌ All benchmark iterations failed\")\n",
        "\n",
        "    def check_system_status(self):\n",
        "        \"\"\"Check system and model status\"\"\"\n",
        "        print(\"\\n🔍 System Status Check:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        print(f\"PyTorch Version: {torch.__version__}\")\n",
        "        print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "            print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
        "            print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
        "            print(f\"GPU Name: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "            # Check GPU memory\n",
        "            try:\n",
        "                memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "                memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "\n",
        "                print(f\"GPU Memory - Allocated: {memory_allocated:.2f}GB\")\n",
        "                print(f\"GPU Memory - Reserved: {memory_reserved:.2f}GB\")\n",
        "                print(f\"GPU Memory - Total: {memory_total:.2f}GB\")\n",
        "            except:\n",
        "                print(\"⚠️ Could not get GPU memory info\")\n",
        "\n",
        "        print(f\"Model Device: {self.device}\")\n",
        "        print(f\"Model Path: {self.model_path}\")\n",
        "\n",
        "# FIXED: Main function with comprehensive error handling\n",
        "def main():\n",
        "    print(\"🚀 Testing Fine-Tuned Fleet Maintenance Model\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Initialize inference class\n",
        "        inference = FixedFleetMaintenanceInference()\n",
        "\n",
        "        # Check system status\n",
        "        inference.check_system_status()\n",
        "\n",
        "        # Run tests with error handling\n",
        "        inference.test_custom_inputs()\n",
        "        inference.test_dataset_examples()\n",
        "        inference.benchmark_performance()\n",
        "\n",
        "        print(\"\\n✅ Testing completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Fatal error in main: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
